{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering\n",
    "This is a very brief and simple demo of spectral clustering for graphs. \n",
    "None of the code below is particularly optimized in any way for running time, memory etc.\n",
    "\n",
    "(c) M. Schaub, IDSS, Massachusetts Institute of Technology\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This code is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
    "http://www.gnu.org/licenses/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing some libraries first \n",
    "#networks library\n",
    "import networkx as nx             \n",
    "#a rule to show plots inside browser\n",
    "%matplotlib inline\n",
    "#plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "#linear algebra etc. --- importing with * is for convenience, even though not recommeded!\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "print 'Libraries imported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_block_model_graph(affinity_matrix, nr_nodes=100):\n",
    "    # affinity matrix, partition_vec and degree_seq should all be numpy arrays\n",
    "    # UNCORRECTED SBM -- affinity matrix should specify the link probability between each block.\n",
    "    \n",
    "    nr_groups = affinity_matrix.shape[0]\n",
    "    group_size = nr_nodes / nr_groups\n",
    "    rem = nr_nodes % nr_groups\n",
    "    group_size_vec= np.ones(nr_groups,dtype=int)*group_size\n",
    "    group_size_vec[-1] += rem\n",
    "    \n",
    "    # true partition vector\n",
    "    partition_vec = np.hstack([np.zeros((1,rem)), np.kron(np.arange(nr_groups),np.ones((1,group_size)))]).flatten()\n",
    "    \n",
    "    data = np.tile(1,(nr_nodes,1)).reshape(nr_nodes)\n",
    "    partition_matrix = sparse.coo_matrix((data,(np.arange(nr_nodes),partition_vec))).toarray()\n",
    "    \n",
    "    P = partition_matrix.dot(affinity_matrix).dot(partition_matrix.T)\n",
    "    \n",
    "    G=nx.Graph()\n",
    "    # make sure that all nodes are added even though the graph might be disconnected\n",
    "    G.add_nodes_from(np.arange(nr_nodes))\n",
    "            \n",
    "\n",
    "    #cycle through nodes and generate edges\n",
    "    for i in range(nr_groups):\n",
    "        for j in range(i,nr_groups):\n",
    "            p = affinity_matrix[i,j]\n",
    "            # sample edges in each block at random as a vector, reshape it to a matrix\n",
    "            # and then construct a network from it\n",
    "            edge_vec = np.random.rand((group_size_vec[i]*group_size_vec[j])) < p\n",
    "            edges = edge_vec.reshape((group_size_vec[i],group_size_vec[j]))\n",
    "            if i == j:\n",
    "                edges = np.triu(edges)\n",
    "            edges = edges.nonzero()\n",
    "            G.add_edges_from(zip(i*group_size+edges[0],j*group_size+edges[1]))\n",
    "\n",
    "        #remove self loops\n",
    "        G.remove_edges_from(G.selfloop_edges())\n",
    "    return G, P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: stochastic blockmodel\n",
    "A stochastic blockmodel (SBM) is a generative model for a network. \n",
    "The model is specified as follows.\n",
    "Each node $i$ is endowed with a (latent) group label $g_i = \\{1,\\ldots,k\\}$.\n",
    "The probability for two nodes $i, j$ to connect, conditional on the group labels, is given by:\n",
    "\n",
    "$$\\mathbb P(A_{ij} = 1 | g_i, g_j) = \\omega_{g_i g_j}$$\n",
    "\n",
    "where we have defined the affinity matrix $(\\omega_{ij})$, where each entry $\\omega_{ij}\\in [0,1]$. \n",
    "Further, all edges are (conditionally) idependently distributed.\n",
    "\n",
    "In our example below we will focus on the case where the diagonal entries of $\\omega$ are larger than the off-diagonal terms ('clustering')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's create a graph with blockmodel structure\n",
    "affinity = np.array([[0.8,.05, 0],[.05,0.8, 0.1],[0, 0.1,0.7]])\n",
    "print affinity\n",
    "\n",
    "G, P = create_block_model_graph(affinity, nr_nodes=500)\n",
    "position=nx.spring_layout(G,scale=5)\n",
    "# drawing may be slow..\n",
    "nx.draw_networkx(G, pos=position, cmap=plt.get_cmap('jet')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the $\\{0,1\\}$ partition indicator matrices $H_{ij} = 1$ if node $i$ is in group j, and zero otherwise.\n",
    "The expected adjacency matrix for our graph (conditioned on the group labels) is now:\n",
    "\n",
    "$$ \\mathcal A =\\mathbb E [A|\\{g_i\\}] = H \\omega H^\\top$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The expected adjacency matrix looks like this..\n",
    "plt.figure()\n",
    "plt.imshow(P);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering the partitions from the expected adjacency matrix\n",
    "Let's see how we could recover the partition via a spectral algorithm, if we had access to $\\mathcal A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = np.diag(P.sum(axis=1))\n",
    "LP = D - P\n",
    "\n",
    "# first plot --- the spectrum of the expected adjacency matrix -- there are only k nonzero eigenvalues\n",
    "ev, evecs = scipy.linalg.eigh(P)\n",
    "plt.figure()\n",
    "plt.plot(ev);\n",
    "\n",
    "# the corresponding Laplacian spectrum --- now we have to look for the smallest eigenvalues..\n",
    "ev, evecs = scipy.linalg.eigh(LP)\n",
    "plt.figure()\n",
    "plt.plot(ev);\n",
    "\n",
    "# Let us check the corresponding eigenvectors --- there should be k (k=3) eigenvectors which are constant on each group...\n",
    "plt.figure()\n",
    "plt.plot(evecs[:,:4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are indeed 3 piecewise constant eigenvectors -- what now?\n",
    "Let's denote these three vectors by $V_i, i\\in 1,\\ldots, k$ and form the matrix $V = [V_1,\\ldots,V_k]$.\n",
    "Now we assign each node $i$ in the network a coordinate corresponding to the $i$th row of this matrix $V$.\n",
    "(This is a spectral embedding).\n",
    "\n",
    "We can plot the nodes in this lower-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take the 3 eigenvectors and use the entries as coordinates\n",
    "# As there the eigenvectors were constant on the groups, there are only 3 distinct positions a node can occupy\n",
    "X = evecs[:,:3]\n",
    "plt.figure();\n",
    "plt.plot(X[:,1],X[:,2],'rx');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering the partitions from the observed network\n",
    "Of course, we normally not have access to the generative model and the expected network --- what can we do??\n",
    "Let us assume here that the network came from a SBM. Subject to some technical conditions, one can then show that the observed network will have spectral properties that are close to the expected network. So instead of using the eigenvectors of the expected adjacency matrix, we will make use of the observed matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's now do the same thing with the observed (sampled) network\n",
    "A = nx.to_scipy_sparse_matrix(G)\n",
    "D = np.diag(A.sum(axis=0))\n",
    "L = D-A\n",
    "\n",
    "ev, evecs = scipy.linalg.eigh(L)\n",
    "plt.figure();\n",
    "plt.plot(ev);\n",
    "\n",
    "plt.figure();\n",
    "plt.plot(evecs[:,:3]);\n",
    "\n",
    "# Let's take the 3 eigenvectors and use the entries as coordinates\n",
    "# Here we use only the 2nd and 3 dimension again\n",
    "X = evecs[:,:3]\n",
    "plt.figure();\n",
    "plt.plot(X[:,1],X[:,2],'rx');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's do all of this on the observed network, with a 'full' algorithm -- as proposed by Qin and Rohe.\n",
    "# Note that this algorithm uses a few additional tricks, such as regularization (works better for sparse networks),\n",
    "# and is based on the degree corrected blockmodel.\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg as linalg\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "##########################################\n",
    "# REGULARIZED SPECTRAL CLUSTERING (ROHE)\n",
    "##########################################\n",
    "\n",
    "def regularized_laplacian_spectral_clustering(A, num_groups=2, tau=-1):\n",
    "    \"\"\"\n",
    "    Performs regularized spectral clustering based on Qin-Rohe 2013 using a normalized and\n",
    "    regularized adjacency matrix (called Laplacian by Rohe et al)\n",
    "    \"\"\"\n",
    "\n",
    "    A = test_sparse_and_transform(A)\n",
    "\n",
    "    # check if tau regularisation parameter is specified otherwise go for mean degree...\n",
    "    if tau==-1:\n",
    "        # set tau to average degree\n",
    "        tau = A.sum()/A.shape[0]\n",
    "\n",
    "    d = np.array(A.sum(axis=1)).flatten().astype(float)\n",
    "    Dtau_sqrt_inv = scipy.sparse.diags(np.power(d + tau,-.5),0)\n",
    "    L = Dtau_sqrt_inv.dot(A).dot(Dtau_sqrt_inv)\n",
    "\n",
    "\n",
    "    # compute eigenvalues and eigenvectors (sorted according to magnitude first)\n",
    "    ev, evecs = scipy.sparse.linalg.eigsh(L,num_groups,which='LM')\n",
    "\n",
    "    X = preprocessing.normalize(evecs, axis=1, norm='l2')\n",
    "\n",
    "    clust = KMeans(n_clusters = num_groups)\n",
    "    clust.fit(X)\n",
    "    partition_vector = clust.labels_\n",
    "\n",
    "\n",
    "    return partition_vector, evecs\n",
    "\n",
    "def test_sparse_and_transform(A):\n",
    "    \"\"\" Check if matrix is sparse and if not, return it as sparse matrix\"\"\"\n",
    "    if not scipy.sparse.issparse(A):\n",
    "        print \"Input matrix not in sparse format, transforming to sparse matrix\"\n",
    "        A = scipy.sparse.csr_matrix(A)\n",
    "    return A\n",
    "\n",
    "\n",
    "partition,number_partition= regularized_laplacian_spectral_clustering(A, num_groups=3, tau=-1)\n",
    "print \"partition=\", partition\n",
    "nx.draw_networkx(G, pos=position, cmap=plt.get_cmap('prism'),node_color=partition,vmin=partition.min(),vmax=partition.max()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Material to play with \n",
    "\n",
    "Some further spectral clustering code to play with / adapt (make sure to load all necessary libraries first)..\n",
    "For instance one can use the Bethe Hessian to estimate the number of groups present in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# BETHE HESSIAN CLUSTERING\n",
    "######################################\n",
    "\n",
    "def build_BetheHessian(A, r):\n",
    "    \"\"\"\n",
    "    Construct Standard Bethe Hessian as discussed, e.g., in Saade et al\n",
    "    B = (r^2-1)*I-r*A+D\n",
    "    \"\"\"\n",
    "    A = test_sparse_and_transform(A)\n",
    "\n",
    "    d = A.sum(axis=1).getA().flatten().astype(float)\n",
    "    B = scipy.sparse.eye(A.shape[0]).dot(r**2 -1) -r*A +  scipy.sparse.diags(d,0)\n",
    "    return B\n",
    "\n",
    "def cluster_with_BetheHessian(A, num_groups=-1):\n",
    "    \"\"\"\n",
    "    Perform one round of spectral clustering using the Bethe Hessian\n",
    "    Input: adjacency matrix A, number of groups (num_groups; set to -1, if group number\n",
    "    should be inferred automatically)\n",
    "    \"\"\"\n",
    "\n",
    "    # set r to square root of average degree\n",
    "    r = A.sum()/A.shape[0]\n",
    "    r = np.sqrt(r)\n",
    "\n",
    "    # construct both the positive and the negative variant of the BH\n",
    "    if not all(A.sum(axis=1)):\n",
    "        print \"GRAPH CONTAINS NODES WITH DEGREE ZERO\"\n",
    "\n",
    "    BH_pos = build_BetheHessian(A,r)\n",
    "    BH_neg = build_BetheHessian(A,-r)\n",
    "\n",
    "\n",
    "    if num_groups ==-1:\n",
    "        relevant_ev, _ = find_negative_eigenvectors(BH_pos)\n",
    "        X = relevant_ev\n",
    "\n",
    "        relevant_ev, _ = find_negative_eigenvectors(BH_neg)\n",
    "        X = np.hstack([X, relevant_ev])\n",
    "        num_groups = X.shape[1]\n",
    "\n",
    "        if num_groups == 0:\n",
    "            print \"no indication for grouping -- return all in one partition\"\n",
    "            partition_vector = np.zeros(A.shape[0],dtype='int')\n",
    "            return partition_vector\n",
    "\n",
    "    else:\n",
    "        # note that we combine the eigenvectors of pos/negative BH and do not use\n",
    "        # information about positive / negative assortativity here\n",
    "        # find eigenvectors corresponding to the algebraically smallest (most neg.) eigenvalues\n",
    "        ev_pos, evecs_pos = scipy.sparse.linalg.eigsh(BH_pos,num_groups,which='SA')\n",
    "        ev_neg, evecs_neg = scipy.sparse.linalg.eigsh(BH_neg,num_groups,which='SA')\n",
    "        ev_all = np.hstack([ev_pos, ev_neg])\n",
    "        index = np.argsort(ev_all)\n",
    "        X = np.hstack([evecs_pos,evecs_neg])\n",
    "        X = X[:,index]\n",
    "\n",
    "\n",
    "    clust = KMeans(n_clusters = num_groups)\n",
    "    clust.fit(X)\n",
    "    partition_vector = clust.labels_\n",
    "\n",
    "\n",
    "    return partition_vector\n",
    "\n",
    "def find_negative_eigenvectors(M):\n",
    "    \"\"\"\n",
    "    Given a matrix M, find all the eigenvectors associated to negative eigenvalues\n",
    "    and return the tuple (evecs, evalues)\n",
    "    \"\"\"\n",
    "    Kmax = M.shape[0]-1\n",
    "    K = min(10,Kmax)\n",
    "    ev, evecs = scipy.sparse.linalg.eigsh(M,K,which='SA')\n",
    "    relevant_ev = np.nonzero(ev <0)[0]\n",
    "    while (relevant_ev.size  == K):\n",
    "        K = min(2*K, Kmax)\n",
    "        ev, evecs = scipy.sparse.linalg.eigsh(M,K,which='SA')\n",
    "        relevant_ev = np.nonzero(ev<0)[0]\n",
    "\n",
    "    return evecs[:,relevant_ev], ev[relevant_ev]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Some useful References__\n",
    "\n",
    "Below I list merely some references to the algorithms implemented above, and a general tutorial article on spectral clustering. There is a lot more out there to explore..\n",
    "\n",
    "A good overview article is the following\n",
    "\n",
    "* Von Luxburg, Ulrike. \"A tutorial on spectral clustering.\" Statistics and computing 17.4 (2007): 395-416.\n",
    "\n",
    "The above implemented spectral algorithms are described here below\n",
    "\n",
    "* Rohe, Karl, Sourav Chatterjee, and Bin Yu. \"Spectral clustering and the high-dimensional stochastic blockmodel.\" The Annals of Statistics (2011): 1878-1915.\n",
    "\n",
    "* Qin, Tai, and Karl Rohe. \"Regularized spectral clustering under the degree-corrected stochastic blockmodel.\" Advances in Neural Information Processing Systems. 2013.\n",
    "\n",
    "* Saade, Alaa, Florent Krzakala, and Lenka ZdeborovÃ¡. \"Spectral clustering of graphs with the bethe hessian.\" Advances in Neural Information Processing Systems. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
